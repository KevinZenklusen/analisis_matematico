{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml\n",
    "import numpy as np\n",
    "from AMIA_part1 import QDA, LDA, TensorizedQDA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from AMIA_part1 import QDA, LDA, TensorizedQDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seed = 8761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos datos de IRIS:\n",
    "def get_iris_dataset():\n",
    "    data = load_iris()#CARGAMSO DATOS\n",
    "    X_full = data.data\n",
    "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "    return X_full, y_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos datos de penguins:\n",
    "def get_penguins():\n",
    "    # get data\n",
    "    #La función fetch_openml se utiliza para descargar conjuntos de datos de la biblioteca en línea OpenML, este es un repositorio de conjuntos de datos.\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True)\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    #isna: devuelve una estructura de datos del mismo tamaño que la entrada, pero con valores booleanos que indican si cada elemento en la entrada es un valor faltante (True) o no (False).\n",
    "    #con .sum(axis=1) hacemos la suma a lo largo de cada columna.\n",
    "    ##==0, compara cada resultado de la suma con cero. Esto devuelve un DataFrame de booleanos (True o False) donde True indica que la suma de valores faltantes en la fila es igual a cero, es decir, la fila no tiene valores faltantes.\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pedimos los datos de iris:\n",
    "X_full, y_full = get_iris_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividimos los datos:\n",
    "def split_transpose(X, y, test_sz, random_state):\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.3, random_state=rng_seed)\n",
    "\n",
    "    # transpose so observations are column vectors\n",
    "    return X_train.T, y_train.T, X_test.T, y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#función para calcular Accuracy:\n",
    "def accuracy(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transponemos datos:\n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREAMOS DATA FRAME PARA ALMACENAR LOS DATOS:\n",
    "df_results = pd.DataFrame(index=[], columns=[\"Modelo\",\t\"Dataset\",\t\"Seed\",\t\"Error (train)\",\t\"Error (test)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribuciones a priori uniforme:\n",
    "priori_prob = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "qda_uniform = QDA()\n",
    "\n",
    "qda_uniform.fit(train_x, train_y, priori_prob)\n",
    "\n",
    "train_acc = accuracy(train_y, qda_uniform.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda_uniform.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos datos en el dataframe:\n",
    "df_results.at[0, \"Modelo\"] = \"QDAUniform\"\n",
    "df_results.at[0, \"Dataset\"] = \"Iris\"\n",
    "df_results.at[0, \"Seed\"] = rng_seed\n",
    "df_results.at[0, \"Error (train)\"] = 1-train_acc\n",
    "df_results.at[0, \"Error (test)\"] = 1-test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
    "#primer combinacion:\n",
    "priori_prob = np.array([0.9, 0.05, 0.05])\n",
    "\n",
    "qda_differentprob = QDA()\n",
    "\n",
    "qda_differentprob.fit(train_x, train_y, priori_prob)\n",
    "\n",
    "train_acc = accuracy(train_y, qda_differentprob.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda_differentprob.predict(test_x))\n",
    "\n",
    "\n",
    "df_results.at[1, \"Modelo\"] = \"QDA with [0.9, 0.05, 0.05]\"\n",
    "df_results.at[1, \"Dataset\"] = \"Iris\"\n",
    "df_results.at[1, \"Seed\"] = rng_seed\n",
    "df_results.at[1, \"Error (train)\"] = 1-train_acc\n",
    "df_results.at[1, \"Error (test)\"] = 1-test_acc\n",
    "\n",
    "#segunda combinacion:\n",
    "priori_prob = np.array([0.05, 0.9, 0.05])\n",
    "\n",
    "qda_differentprob = QDA()\n",
    "\n",
    "qda_differentprob.fit(train_x, train_y, priori_prob)\n",
    "\n",
    "train_acc = accuracy(train_y, qda_differentprob.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda_differentprob.predict(test_x))\n",
    "\n",
    "\n",
    "df_results.at[2, \"Modelo\"] = \"QDA with [0.05, 0.9, 0.05]\"\n",
    "df_results.at[2, \"Dataset\"] = \"Iris\"\n",
    "df_results.at[2, \"Seed\"] = rng_seed\n",
    "df_results.at[2, \"Error (train)\"] = 1-train_acc\n",
    "df_results.at[2, \"Error (test)\"] = 1-test_acc\n",
    "\n",
    "#tercera combinacion:\n",
    "priori_prob = np.array([0.05, 0.05, 0.9])\n",
    "\n",
    "qda_differentprob = QDA()\n",
    "\n",
    "qda_differentprob.fit(train_x, train_y, priori_prob)\n",
    "\n",
    "train_acc = accuracy(train_y, qda_differentprob.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda_differentprob.predict(test_x))\n",
    "\n",
    "\n",
    "df_results.at[3, \"Modelo\"] = \"QDA with [0.05, 0.05, 0.9]\"\n",
    "df_results.at[3, \"Dataset\"] = \"Iris\"\n",
    "df_results.at[3, \"Seed\"] = rng_seed\n",
    "df_results.at[3, \"Error (train)\"] = 1-train_acc\n",
    "df_results.at[3, \"Error (test)\"] = 1-test_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franco\\OneDrive\\Escritorio\\ESPECIALIZACIO EN INTELIGENCIA ARTIFICIAL\\INTRO IA\\clase1\\EV_IA\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#LO MISMO PERO PARA PENGUINS:\n",
    "#pedimos los datos a penguins:\n",
    "X_penguins_full, y_penguins_full = get_penguins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transponemos datos:\n",
    "penguins_train_x, penguins_train_y, penguins_test_x, penguins_test_y = split_transpose(X_full, y_full, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caso uniforme:\n",
    "priori_prob = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "qda_uniform_penguins = QDA()\n",
    "\n",
    "qda_uniform_penguins.fit(penguins_train_x, penguins_train_y, priori_prob)\n",
    "\n",
    "penguins_train_acc = accuracy(penguins_train_y, qda_uniform.predict(penguins_train_x))\n",
    "penguins_test_acc = accuracy(penguins_test_y, qda_uniform.predict(penguins_test_x))\n",
    "\n",
    "df_results.at[4, \"Modelo\"] = \"QDAUniform\"\n",
    "df_results.at[4, \"Dataset\"] = \"Penguins\"\n",
    "df_results.at[4, \"Seed\"] = rng_seed\n",
    "df_results.at[4, \"Error (train)\"] = 1-penguins_train_acc\n",
    "df_results.at[4, \"Error (test)\"] = 1-penguins_test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
    "#primer combinacion:\n",
    "priori_prob = np.array([0.9, 0.05, 0.05])\n",
    "\n",
    "qda_penguins = QDA()\n",
    "\n",
    "qda_penguins.fit(penguins_train_x, penguins_train_y, priori_prob)\n",
    "\n",
    "penguins_train_acc = accuracy(penguins_train_y, qda_penguins.predict(penguins_train_x))\n",
    "penguins_test_acc = accuracy(penguins_test_y, qda_penguins.predict(penguins_test_x))\n",
    "\n",
    "df_results.at[5, \"Modelo\"] = \"QDA with [0.9, 0.05, 0.05]\"\n",
    "df_results.at[5, \"Dataset\"] = \"Penguins\"\n",
    "df_results.at[5, \"Seed\"] = rng_seed\n",
    "df_results.at[5, \"Error (train)\"] = 1-penguins_train_acc\n",
    "df_results.at[5, \"Error (test)\"] = 1-penguins_test_acc\n",
    "\n",
    "#segunda combinacion:\n",
    "priori_prob = np.array([0.05, 0.9, 0.05])\n",
    "\n",
    "qda_penguins = QDA()\n",
    "\n",
    "qda_penguins.fit(penguins_train_x, penguins_train_y, priori_prob)\n",
    "\n",
    "penguins_train_acc = accuracy(penguins_train_y, qda_penguins.predict(penguins_train_x))\n",
    "penguins_test_acc = accuracy(penguins_test_y, qda_penguins.predict(penguins_test_x))\n",
    "\n",
    "df_results.at[6, \"Modelo\"] = \"QDA with [0.05, 0.9, 0.05]\"\n",
    "df_results.at[6, \"Dataset\"] = \"Penguins\"\n",
    "df_results.at[6, \"Seed\"] = rng_seed\n",
    "df_results.at[6, \"Error (train)\"] = 1-penguins_train_acc\n",
    "df_results.at[6, \"Error (test)\"] = 1-penguins_test_acc\n",
    "\n",
    "#tercera combinacion:\n",
    "priori_prob = np.array([0.05, 0.05, 0.9])\n",
    "\n",
    "qda_penguins = QDA()\n",
    "\n",
    "qda_penguins.fit(penguins_train_x, penguins_train_y, priori_prob)\n",
    "\n",
    "penguins_train_acc = accuracy(penguins_train_y, qda_penguins.predict(penguins_train_x))\n",
    "penguins_test_acc = accuracy(penguins_test_y, qda_penguins.predict(penguins_test_x))\n",
    "\n",
    "df_results.at[7, \"Modelo\"] = \"QDA with [0.05, 0.05, 0.9]\"\n",
    "df_results.at[7, \"Dataset\"] = \"Penguins\"\n",
    "df_results.at[7, \"Seed\"] = rng_seed\n",
    "df_results.at[7, \"Error (train)\"] = 1-penguins_train_acc\n",
    "df_results.at[7, \"Error (test)\"] = 1-penguins_test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo lo anterior pero con LDA:\n",
    "\n",
    "#caso uniforme\n",
    "priori_prob = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "lda_uniform = LDA()\n",
    "\n",
    "lda_uniform.fit(train_x, train_y, priori_prob)\n",
    "\n",
    "lda_train_acc = accuracy(train_y, lda_uniform.predict(train_x))\n",
    "lda_test_acc = accuracy(test_y, lda_uniform.predict(test_x))\n",
    "\n",
    "df_results.at[8, \"Modelo\"] = \"LDAUniform\"\n",
    "df_results.at[8, \"Dataset\"] = \"Iris\"\n",
    "df_results.at[8, \"Seed\"] = rng_seed\n",
    "df_results.at[8, \"Error (train)\"] = 1-lda_train_acc\n",
    "df_results.at[8, \"Error (test)\"] = 1-lda_test_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
    "#primer combinacion:\n",
    "priori_prob = np.array([0.9, 0.05, 0.05])\n",
    "\n",
    "lda_differentprob = LDA()\n",
    "\n",
    "lda_differentprob.fit(train_x, train_y, priori_prob)\n",
    "\n",
    "lda_train_acc = accuracy(train_y, lda_differentprob.predict(train_x))\n",
    "lda_test_acc = accuracy(test_y, lda_differentprob.predict(test_x))\n",
    "\n",
    "df_results.at[9, \"Modelo\"] = \"LDA with [0.9, 0.05, 0.05]\"\n",
    "df_results.at[9, \"Dataset\"] = \"Iris\"\n",
    "df_results.at[9, \"Seed\"] = rng_seed\n",
    "df_results.at[9, \"Error (train)\"] = 1-lda_train_acc\n",
    "df_results.at[9, \"Error (test)\"] = 1-lda_test_acc\n",
    "\n",
    "#segunda combinacion:\n",
    "priori_prob = np.array([0.05, 0.9, 0.05])\n",
    "\n",
    "lda_differentprob = LDA()\n",
    "\n",
    "lda_differentprob.fit(train_x, train_y, priori_prob)\n",
    "\n",
    "lda_train_acc = accuracy(train_y, lda_differentprob.predict(train_x))\n",
    "lda_test_acc = accuracy(test_y, lda_differentprob.predict(test_x))\n",
    "\n",
    "df_results.at[10, \"Modelo\"] = \"LDA with [0.05, 0.9, 0.05]\"\n",
    "df_results.at[10, \"Dataset\"] = \"Iris\"\n",
    "df_results.at[10, \"Seed\"] = rng_seed\n",
    "df_results.at[10, \"Error (train)\"] = 1-lda_train_acc\n",
    "df_results.at[10, \"Error (test)\"] = 1-lda_test_acc\n",
    "\n",
    "\n",
    "#tercera combinacion:\n",
    "priori_prob = np.array([0.05, 0.05, 0.9])\n",
    "\n",
    "lda_differentprob = LDA()\n",
    "\n",
    "lda_differentprob.fit(train_x, train_y, priori_prob)\n",
    "\n",
    "lda_train_acc = accuracy(train_y, lda_differentprob.predict(train_x))\n",
    "lda_test_acc = accuracy(test_y, lda_differentprob.predict(test_x))\n",
    "\n",
    "df_results.at[11, \"Modelo\"] = \"LDA with [0.05, 0.05, 0.9]\"\n",
    "df_results.at[11, \"Dataset\"] = \"Iris\"\n",
    "df_results.at[11, \"Seed\"] = rng_seed\n",
    "df_results.at[11, \"Error (train)\"] = 1-lda_train_acc\n",
    "df_results.at[11, \"Error (test)\"] = 1-lda_test_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0286 while test error is 0.0000\n"
     ]
    }
   ],
   "source": [
    "#LDA PARA PINGUINS:\n",
    "lda_uniform_penguins = LDA()\n",
    "\n",
    "lda_uniform_penguins.fit(penguins_train_x, penguins_train_y)\n",
    "\n",
    "penguins_train_acc = accuracy(penguins_train_y, qda_uniform.predict(penguins_train_x))\n",
    "penguins_test_acc = accuracy(penguins_test_y, qda_uniform.predict(penguins_test_x))\n",
    "print(f\"Train (apparent) error is {1-penguins_train_acc:.4f} while test error is {1-penguins_test_acc:.4f}\")\n",
    "\n",
    "df_results.at[12, \"Modelo\"] = \"LDAUniform\"\n",
    "df_results.at[12, \"Dataset\"] = \"Penguins\"\n",
    "df_results.at[12, \"Seed\"] = rng_seed\n",
    "df_results.at[12, \"Error (train)\"] = 1-penguins_train_acc\n",
    "df_results.at[12, \"Error (test)\"] = 1-penguins_test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Modelo   Dataset  Seed Error (train) Error (test)\n",
      "0                   QDAUniform      Iris  8761      0.028571          0.0\n",
      "1   QDA with [0.9, 0.05, 0.05]      Iris  8761      0.028571          0.0\n",
      "2   QDA with [0.05, 0.9, 0.05]      Iris  8761       0.07619     0.022222\n",
      "3   QDA with [0.05, 0.05, 0.9]      Iris  8761      0.057143          0.0\n",
      "4                   QDAUniform  Penguins  8761      0.028571          0.0\n",
      "5   QDA with [0.9, 0.05, 0.05]  Penguins  8761      0.028571          0.0\n",
      "6   QDA with [0.05, 0.9, 0.05]  Penguins  8761       0.07619     0.022222\n",
      "7   QDA with [0.05, 0.05, 0.9]  Penguins  8761      0.057143          0.0\n",
      "8                   LDAUniform      Iris  8761      0.028571          0.0\n",
      "9   LDA with [0.9, 0.05, 0.05]      Iris  8761      0.685714     0.622222\n",
      "10  LDA with [0.05, 0.9, 0.05]      Iris  8761      0.685714     0.622222\n",
      "11  LDA with [0.05, 0.05, 0.9]      Iris  8761      0.628571     0.755556\n",
      "12                  LDAUniform  Penguins  8761      0.028571          0.0\n",
      "13  LDA with [0.9, 0.05, 0.05]  Penguins  8761      0.685714     0.622222\n",
      "14  LDA with [0.05, 0.9, 0.05]  Penguins  8761      0.685714     0.622222\n",
      "15  LDA with [0.05, 0.05, 0.9]  Penguins  8761      0.628571     0.755556\n"
     ]
    }
   ],
   "source": [
    "#Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
    "#primer combinacion:\n",
    "priori_prob = np.array([0.9, 0.05, 0.05])\n",
    "\n",
    "lda_differentprob_penguins = LDA()\n",
    "\n",
    "lda_differentprob_penguins.fit(penguins_train_x, penguins_train_y, priori_prob)\n",
    "\n",
    "penguins_train_acc = accuracy(penguins_train_y, lda_differentprob_penguins.predict(penguins_train_x))\n",
    "penguins_test_acc = accuracy(penguins_test_y, lda_differentprob_penguins.predict(penguins_test_x))\n",
    "\n",
    "df_results.at[13, \"Modelo\"] = \"LDA with [0.9, 0.05, 0.05]\"\n",
    "df_results.at[13, \"Dataset\"] = \"Penguins\"\n",
    "df_results.at[13, \"Seed\"] = rng_seed\n",
    "df_results.at[13, \"Error (train)\"] = 1-penguins_train_acc\n",
    "df_results.at[13, \"Error (test)\"] = 1-penguins_test_acc\n",
    "\n",
    "#segunda combinacion:\n",
    "priori_prob = np.array([0.05, 0.9, 0.05])\n",
    "\n",
    "lda_differentprob_penguins = LDA()\n",
    "\n",
    "lda_differentprob_penguins.fit(penguins_train_x, penguins_train_y, priori_prob)\n",
    "\n",
    "penguins_train_acc = accuracy(penguins_train_y, lda_differentprob_penguins.predict(penguins_train_x))\n",
    "penguins_test_acc = accuracy(penguins_test_y, lda_differentprob_penguins.predict(penguins_test_x))\n",
    "\n",
    "df_results.at[14, \"Modelo\"] = \"LDA with [0.05, 0.9, 0.05]\"\n",
    "df_results.at[14, \"Dataset\"] = \"Penguins\"\n",
    "df_results.at[14, \"Seed\"] = rng_seed\n",
    "df_results.at[14, \"Error (train)\"] = 1-penguins_train_acc\n",
    "df_results.at[14, \"Error (test)\"] = 1-penguins_test_acc\n",
    "\n",
    "#tercera combinacion:\n",
    "priori_prob = np.array([0.05, 0.05, 0.9])\n",
    "\n",
    "lda_differentprob_penguins = LDA()\n",
    "\n",
    "lda_differentprob_penguins.fit(penguins_train_x, penguins_train_y, priori_prob)\n",
    "\n",
    "penguins_train_acc = accuracy(penguins_train_y, lda_differentprob_penguins.predict(penguins_train_x))\n",
    "penguins_test_acc = accuracy(penguins_test_y, lda_differentprob_penguins.predict(penguins_test_x))\n",
    "\n",
    "df_results.at[15, \"Modelo\"] = \"LDA with [0.05, 0.05, 0.9]\"\n",
    "df_results.at[15, \"Dataset\"] = \"Penguins\"\n",
    "df_results.at[15, \"Seed\"] = rng_seed\n",
    "df_results.at[15, \"Error (train)\"] = 1-penguins_train_acc\n",
    "df_results.at[15, \"Error (test)\"] = 1-penguins_test_acc\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se asigna una probabilidad alta (0.9) a una clase específica y probabilidades bajas (0.05) a las otras dos clases, el modelo parece aprender bien esa clase, ya que el error de test es bajo. La clase con probabilidad alta es más fácil de aprender.\n",
    "\n",
    "Cuando la probabilidad alta se asigna a una clase diferente ([0.05, 0.05, 0.9]), el error de test aumenta, el modelo tiene más dificultades con esa clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EV_IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
